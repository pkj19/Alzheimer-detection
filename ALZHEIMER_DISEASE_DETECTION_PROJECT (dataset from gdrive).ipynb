{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3i3eaV9j3-kd",
        "outputId": "ab498214-acff-42d7-ee2b-b98bfdef64a3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ba6d1f22b251>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/oasis_longitudinal.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/oasis_longitudinal.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set()\n",
        "\n",
        "df = pd.read_csv('/oasis_longitudinal.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H5DzGLR1IrGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "26832137-3431-4e5f-c125-a6291277edaf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOLJMKjL5kML"
      },
      "outputs": [],
      "source": [
        "df = df.loc[df['Visit']==1] # use first visit data only because of the analysis we're doing\n",
        "df = df.reset_index(drop=True) # reset index after filtering first visit data\n",
        "df['M/F'] = df['M/F'].replace(['F','M'], [0,1]) # M/F column\n",
        "df['Group'] = df['Group'].replace(['Converted'], ['Demented']) # Target variable\n",
        "df['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0]) # Target variable\n",
        "df = df.drop(['MRI ID', 'Visit', 'Hand'], axis=1) # Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKX6Oc2P53In"
      },
      "outputs": [],
      "source": [
        "# bar drawing function\n",
        "def bar_chart(feature):\n",
        "    Demented = df[df['Group']==1][feature].value_counts()\n",
        "    Nondemented = df[df['Group']==0][feature].value_counts()\n",
        "    df_bar = pd.DataFrame([Demented,Nondemented])\n",
        "    df_bar.index = ['Demented','Nondemented']\n",
        "    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJqV4cZI6CGO"
      },
      "outputs": [],
      "source": [
        "# Gender  and  Group ( Femal=0, Male=1)\n",
        "bar_chart('M/F')\n",
        "plt.xlabel('Group')\n",
        "plt.ylabel('Number of patients')\n",
        "plt.legend()\n",
        "plt.title('Gender and Demented rate')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58eEe1NU8C3U"
      },
      "source": [
        "                  The above graph indicates that men are more likely with dementia than women.bold text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWCyIE2g6Xm-"
      },
      "outputs": [],
      "source": [
        "#MMSE : Mini Mental State Examination\n",
        "# Nondemented = 0, Demented =1\n",
        "# Nondemented has higher test result ranging from 25 to 30. \n",
        "#Min 17 ,MAX 30\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'MMSE',shade= True)\n",
        "facet.set(xlim=(0, df['MMSE'].max()))\n",
        "facet.add_legend()\n",
        "plt.xlim(15.30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R4H9GAQ8dSn"
      },
      "source": [
        "               The chart shows Nondemented group got much more higher MMSE scores than Demented group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2sY3zOH6hVT"
      },
      "outputs": [],
      "source": [
        "#bar_chart('ASF') = Atlas Scaling Factor\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'ASF',shade= True)\n",
        "facet.set(xlim=(0, df['ASF'].max()))\n",
        "facet.add_legend()\n",
        "plt.xlim(0.5, 2)\n",
        "\n",
        "#eTIV = Estimated Total Intracranial Volume\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'eTIV',shade= True)\n",
        "facet.set(xlim=(0, df['eTIV'].max()))\n",
        "facet.add_legend()\n",
        "plt.xlim(900, 2100)\n",
        "\n",
        "#'nWBV' = Normalized Whole Brain Volume\n",
        "# Nondemented = 0, Demented =1\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'nWBV',shade= True)\n",
        "facet.set(xlim=(0, df['nWBV'].max()))\n",
        "facet.add_legend()\n",
        "plt.xlim(0.6,0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CG3IbG58ra_"
      },
      "source": [
        "                       The chart indicates that Nondemented group has higher brain volume ratio than Demented group.\n",
        "                        This is assumed to be because the diseases affect the brain to be shrinking its tissue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ1Jwl3-6r0B"
      },
      "outputs": [],
      "source": [
        "#AGE. Nondemented =0, Demented =0\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'Age',shade= True)\n",
        "facet.set(xlim=(0, df['Age'].max()))\n",
        "facet.add_legend()\n",
        "plt.xlim(50,100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64AKdr469CRe"
      },
      "source": [
        "There is a higher concentration of 70-80 years old in the Demented patient group than those in the\n",
        "     nondemented patients. We guess patients who suffered from that kind of disease has lower survival rate so \n",
        "     that there are a few of 90 years old.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZzVHi9e7Kz6"
      },
      "outputs": [],
      "source": [
        "#'EDUC' = Years of Education\n",
        "# Nondemented = 0, Demented =1\n",
        "facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\n",
        "facet.map(sns.kdeplot,'EDUC',shade= True)\n",
        "facet.set(xlim=(df['EDUC'].min(), df['EDUC'].max()))\n",
        "facet.add_legend()\n",
        "plt.ylim(0, 0.16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl2-yARc9lvV"
      },
      "source": [
        "# **Intermediate Result Summary**\n",
        "1 Men are more likely with demented, an Alzheimer's Disease, than Women.\n",
        "\n",
        "2 Demented patients were less educated in terms of years of education.\n",
        "\n",
        "3 Nondemented group has higher brain volume than Demented group.\n",
        "\n",
        "4 Higher concentration of 70-80 years old in Demented group than those in the nondemented patients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFb73pMP-Mlh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKE5HWAb7XwI"
      },
      "outputs": [],
      "source": [
        "# Check missing values by each column\n",
        "pd.isnull(df).sum() \n",
        "# The column, SES has 8 missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_stFYDQH-7ki"
      },
      "source": [
        "## **5.A Removing rows with missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo9McPwF7gY1"
      },
      "outputs": [],
      "source": [
        "# Dropped the 8 rows with missing values in the column, SES\n",
        "df_dropna = df.dropna(axis=0, how='any')\n",
        "pd.isnull(df_dropna).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHIn7C3n_Ivf"
      },
      "outputs": [],
      "source": [
        "df_dropna['Group'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQTEiWp_Si7"
      },
      "source": [
        "# **Imputation**\n",
        "Scikit-learn provides package for imputation [6], but we do it manually. Since the SES is a discrete variable, we use median for the imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2Mh_OLcRWWx"
      },
      "outputs": [],
      "source": [
        "# Draw scatter plot between EDUC and SES\n",
        "x = df['EDUC']\n",
        "y = df['SES']\n",
        "\n",
        "ses_not_null_index = y[~y.isnull()].index\n",
        "x = x[ses_not_null_index]\n",
        "y = y[ses_not_null_index]\n",
        "\n",
        "# Draw trend line in red\n",
        "z = np.polyfit(x, y, 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(x, y, 'go', x, p(x), \"r--\")\n",
        "plt.xlabel('Education Level(EDUC)')\n",
        "plt.ylabel('Social Economic Status(SES)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEtp8TqHSfO5"
      },
      "outputs": [],
      "source": [
        "df.groupby(['EDUC'])['SES'].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTsa9XItSoN-"
      },
      "outputs": [],
      "source": [
        "df[\"SES\"].fillna(df.groupby(\"EDUC\")[\"SES\"].transform(\"median\"), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUCaDX9iSyO2"
      },
      "outputs": [],
      "source": [
        "# I confirm there're no more missing values and all the 150 data were used.\n",
        "pd.isnull(df['SES']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qNvzJlrTyHz"
      },
      "source": [
        "# **Splitting Train/Validation/Test Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxCUUtmyUFQ3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrXx4WPBULy0"
      },
      "outputs": [],
      "source": [
        "# Dataset with imputation\n",
        "Y = df['Group'].values # Target for the model\n",
        "X = df[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n",
        "\n",
        "# splitting into three sets\n",
        "X_trainval, X_test, Y_trainval, Y_test = train_test_split(\n",
        "    X, Y, random_state=0)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = MinMaxScaler().fit(X_trainval)\n",
        "X_trainval_scaled = scaler.transform(X_trainval)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGmE0vMPUVwK"
      },
      "outputs": [],
      "source": [
        "# Dataset after dropping missing value rows\n",
        "Y = df_dropna['Group'].values # Target for the model\n",
        "X = df_dropna[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n",
        "\n",
        "# splitting into three sets\n",
        "X_trainval_dna, X_test_dna, Y_trainval_dna, Y_test_dna = train_test_split(\n",
        "    X, Y, random_state=0)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = MinMaxScaler().fit(X_trainval_dna)\n",
        "X_trainval_scaled_dna = scaler.transform(X_trainval_dna)\n",
        "X_test_scaled_dna = scaler.transform(X_test_dna)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXyqlo8wUfqm"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLrNX0ibU3P6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAOQBfVRVM9X"
      },
      "source": [
        "# **Logistic Regression**\n",
        "The parameter C, inverse of regularization strength.\n",
        "\n",
        "Tuning range: [0.001, 0.1, 1, 10, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqQ-OhmuVb-I"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1sU6A_eVg09"
      },
      "outputs": [],
      "source": [
        "acc = [] # list to store all performance metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myoE5o8dWBa0"
      },
      "outputs": [],
      "source": [
        "# Dataset with imputation\n",
        "best_score=0\n",
        "kfolds=5 # set the number of folds\n",
        "\n",
        "for c in [0.001, 0.1, 1, 10, 100]:\n",
        "    logRegModel = LogisticRegression(C=c)\n",
        "    # perform cross-validation\n",
        "    scores = cross_val_score(logRegModel, X_trainval, Y_trainval, cv=kfolds, scoring='accuracy') # Get recall for each parameter setting\n",
        "    \n",
        "    # compute mean cross-validation accuracy\n",
        "    score = np.mean(scores)\n",
        "    \n",
        "    # Find the best parameters and score\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_parameters = c\n",
        "\n",
        "# rebuild a model on the combined training and validation set\n",
        "SelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled, Y_trainval)\n",
        "\n",
        "test_score = SelectedLogRegModel.score(X_test_scaled, Y_test)\n",
        "PredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on validation set is:\", best_score)\n",
        "print(\"Best parameter for regularization (C) is: \", best_parameters)\n",
        "print(\"Test accuracy with best C parameter is\", test_score)\n",
        "print(\"Test recall with the best C parameter is\", test_recall)\n",
        "print(\"Test AUC with the best C parameter is\", test_auc)\n",
        "m = 'Logistic Regression (w/ imputation)'\n",
        "acc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nkYMhuKWfTt"
      },
      "outputs": [],
      "source": [
        "# Dataset after dropping missing value rows\n",
        "best_score=0\n",
        "kfolds=5 # set the number of folds\n",
        "\n",
        "for c in [0.001, 0.1, 1, 10, 100]:\n",
        "    logRegModel = LogisticRegression(C=c)\n",
        "    # perform cross-validation\n",
        "    scores = cross_val_score(logRegModel, X_trainval_scaled_dna, Y_trainval_dna, cv=kfolds, scoring='accuracy')\n",
        "    \n",
        "    # compute mean cross-validation accuracy\n",
        "    score = np.mean(scores)\n",
        "    \n",
        "    # Find the best parameters and score\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_parameters = c\n",
        "\n",
        "# rebuild a model on the combined training and validation set\n",
        "SelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled_dna, Y_trainval_dna)\n",
        "\n",
        "test_score = SelectedLogRegModel.score(X_test_scaled_dna, Y_test_dna)\n",
        "PredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on validation set is:\", best_score)\n",
        "print(\"Best parameter for regularization (C) is: \", best_parameters)\n",
        "print(\"Test accuracy with best C parameter is\", test_score)        \n",
        "print(\"Test recall with the best C parameter is\", test_recall)\n",
        "print(\"Test AUC with the best C parameter is\", test_auc)\n",
        "\n",
        "m = 'Logistic Regression (w/ dropna)'\n",
        "acc.append([m, test_score, test_recall, test_recall, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4CMxMuiW97r"
      },
      "source": [
        "In overall, dataset with imputation outperforms the one without imputation. For the later models, we use dataset without imputation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPi3qGPkXEdY"
      },
      "source": [
        "# **SVM**\n",
        "C: Penalty parameter C of the error term. [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "gamma: kernel coefficient. [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "kernel: kernel type. ['rbf', 'linear', 'poly', 'sigmoid']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwoLCGxOXScH"
      },
      "outputs": [],
      "source": [
        "best_score = 0\n",
        "\n",
        "for c_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter C\n",
        "    for gamma_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter gamma\n",
        "        for k_parameter in ['rbf', 'linear', 'poly', 'sigmoid']: # iterate over the values we need to try for the kernel parameter\n",
        "            svmModel = SVC(kernel=k_parameter, C=c_paramter, gamma=gamma_paramter) #define the model\n",
        "            # perform cross-validation\n",
        "            scores = cross_val_score(svmModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n",
        "            # the training set will be split internally into training and cross validation\n",
        "\n",
        "            # compute mean cross-validation accuracy\n",
        "            score = np.mean(scores)\n",
        "            # if we got a better score, store the score and parameters\n",
        "            if score > best_score:\n",
        "                best_score = score #store the score \n",
        "                best_parameter_c = c_paramter #store the parameter c\n",
        "                best_parameter_gamma = gamma_paramter #store the parameter gamma\n",
        "                best_parameter_k = k_parameter\n",
        "            \n",
        "\n",
        "# rebuild a model with best parameters to get score \n",
        "SelectedSVMmodel = SVC(C=best_parameter_c, gamma=best_parameter_gamma, kernel=best_parameter_k).fit(X_trainval_scaled, Y_trainval)\n",
        "\n",
        "test_score = SelectedSVMmodel.score(X_test_scaled, Y_test)\n",
        "PredictedOutput = SelectedSVMmodel.predict(X_test_scaled)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on cross validation set is:\", best_score)\n",
        "print(\"Best parameter for c is: \", best_parameter_c)\n",
        "print(\"Best parameter for gamma is: \", best_parameter_gamma)\n",
        "print(\"Best parameter for kernel is: \", best_parameter_k)\n",
        "print(\"Test accuracy with the best parameters is\", test_score)\n",
        "print(\"Test recall with the best parameters is\", test_recall)\n",
        "print(\"Test recall with the best parameter is\", test_auc)\n",
        "\n",
        "m = 'SVM'\n",
        "acc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTGmwKmXXabe"
      },
      "source": [
        "# **Decision Tree**\n",
        "Maximum depth. [1, 2, ..., 8]\n",
        "\n",
        "8 is the number of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3MVh3iiXkQN"
      },
      "outputs": [],
      "source": [
        "best_score = 0\n",
        "\n",
        "for md in range(1, 9): # iterate different maximum depth values\n",
        "    # train the model\n",
        "    treeModel = DecisionTreeClassifier(random_state=0, max_depth=md, criterion='gini')\n",
        "    # perform cross-validation\n",
        "    scores = cross_val_score(treeModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n",
        "    \n",
        "    # compute mean cross-validation accuracy\n",
        "    score = np.mean(scores)\n",
        "    \n",
        "    # if we got a better score, store the score and parameters\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_parameter = md\n",
        "\n",
        "# Rebuild a model on the combined training and validation set        \n",
        "SelectedDTModel = DecisionTreeClassifier(max_depth=best_parameter).fit(X_trainval_scaled, Y_trainval )\n",
        "\n",
        "test_score = SelectedDTModel.score(X_test_scaled, Y_test)\n",
        "PredictedOutput = SelectedDTModel.predict(X_test_scaled)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on validation set is:\", best_score)\n",
        "print(\"Best parameter for the maximum depth is: \", best_parameter)\n",
        "print(\"Test accuracy with best parameter is \", test_score)\n",
        "print(\"Test recall with best parameters is \", test_recall)\n",
        "print(\"Test AUC with the best parameter is \", test_auc)\n",
        "\n",
        "m = 'Decision Tree'\n",
        "acc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtN3qW0AXsDD"
      },
      "outputs": [],
      "source": [
        "print(\"Feature importance: \")\n",
        "np.array([X.columns.values.tolist(), list(SelectedDTModel.feature_importances_)]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sb1G0dnX_Fd"
      },
      "source": [
        "# **Random Forest Classifier**\n",
        "n_estimators(M): the number of trees in the forest\n",
        "\n",
        "max_features(d): the number of features to consider when looking for the best split\n",
        "\n",
        "max_depth(m): the maximum depth of the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq-1q0wnYJUI"
      },
      "outputs": [],
      "source": [
        "best_score = 0\n",
        "\n",
        "for M in range(2, 15, 2): # combines M trees\n",
        "    for d in range(1, 9): # maximum number of features considered at each split\n",
        "        for m in range(1, 9): # maximum depth of the tree\n",
        "            # train the model\n",
        "            # n_jobs(4) is the number of parallel computing\n",
        "            forestModel = RandomForestClassifier(n_estimators=M, max_features=d, n_jobs=4,\n",
        "                                          max_depth=m, random_state=0)\n",
        "        \n",
        "            # perform cross-validation\n",
        "            scores = cross_val_score(forestModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n",
        "\n",
        "            # compute mean cross-validation accuracy\n",
        "            score = np.mean(scores)\n",
        "\n",
        "            # if we got a better score, store the score and parameters\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_M = M\n",
        "                best_d = d\n",
        "                best_m = m\n",
        "\n",
        "# Rebuild a model on the combined training and validation set        \n",
        "SelectedRFModel = RandomForestClassifier(n_estimators=M, max_features=d,\n",
        "                                          max_depth=m, random_state=0).fit(X_trainval_scaled, Y_trainval )\n",
        "\n",
        "PredictedOutput = SelectedRFModel.predict(X_test_scaled)\n",
        "test_score = SelectedRFModel.score(X_test_scaled, Y_test)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on validation set is:\", best_score)\n",
        "print(\"Best parameters of M, d, m are: \", best_M, best_d, best_m)\n",
        "print(\"Test accuracy with the best parameters is\", test_score)\n",
        "print(\"Test recall with the best parameters is:\", test_recall)\n",
        "print(\"Test AUC with the best parameters is:\", test_auc)\n",
        "\n",
        "m = 'Random Forest'\n",
        "acc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnqESqE2Yt1k"
      },
      "outputs": [],
      "source": [
        "print(\"Feature importance: \")\n",
        "np.array([X.columns.values.tolist(), list(SelectedRFModel.feature_importances_)]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alOoBm2YZfhV"
      },
      "source": [
        "# **AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xjrcfsVZ_dw"
      },
      "outputs": [],
      "source": [
        "best_score = 0\n",
        "\n",
        "for M in range(2, 15, 2): # combines M trees\n",
        "    for lr in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
        "        # train the model\n",
        "        boostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0)\n",
        "\n",
        "        # perform cross-validation\n",
        "        scores = cross_val_score(boostModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n",
        "\n",
        "        # compute mean cross-validation accuracy\n",
        "        score = np.mean(scores)\n",
        "\n",
        "        # if we got a better score, store the score and parameters\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_M = M\n",
        "            best_lr = lr\n",
        "\n",
        "# Rebuild a model on the combined training and validation set        \n",
        "SelectedBoostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0).fit(X_trainval_scaled, Y_trainval )\n",
        "\n",
        "PredictedOutput = SelectedBoostModel.predict(X_test_scaled)\n",
        "test_score = SelectedRFModel.score(X_test_scaled, Y_test)\n",
        "test_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\n",
        "test_auc = auc(fpr, tpr)\n",
        "print(\"Best accuracy on validation set is:\", best_score)\n",
        "print(\"Best parameter of M is: \", best_M)\n",
        "print(\"best parameter of LR is: \", best_lr)\n",
        "print(\"Test accuracy with the best parameter is\", test_score)\n",
        "print(\"Test recall with the best parameters is:\", test_recall)\n",
        "print(\"Test AUC with the best parameters is:\", test_auc)\n",
        "\n",
        "m = 'AdaBoost'\n",
        "acc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eewt9edaJ2e"
      },
      "outputs": [],
      "source": [
        "print(\"Feature importance: \")\n",
        "np.array([X.columns.values.tolist(), list(SelectedBoostModel.feature_importances_)]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYVp97Saa9v"
      },
      "source": [
        "## **CONCLUSION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df34GLSragsh"
      },
      "source": [
        "## **RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rjyRCkTauK5"
      },
      "outputs": [],
      "source": [
        "# Performance Metric for each model\n",
        "result = pd.DataFrame(acc, columns=['Model', 'Accuracy', 'Recall', 'AUC', 'FPR', 'TPR', 'TH'])\n",
        "result[['Model', 'Accuracy', 'Recall', 'AUC']]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}